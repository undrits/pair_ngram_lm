#!/usr/bin/env python

import argparse
import csv
import json
import logging
import os
import pandas as pd

from typing import Dict, List, Tuple

from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

import pynini


def predict_donor(word: str, rus_eng_fst: pynini.Fst) -> Tuple[float, str]:
    """
    Convert a word into its possible donor by composing it with
    the pair Rus-Eng Fst. Return lattice cost and
    a predicted donor

    :param word: string
    :param rus_eng_fst: a pretraimed pynini FST that converts a Rus string into a predicted Eng donor
    :return: lattice cost and a predicted Eng donor
    """

    acceptor = pynini.accep(word, token_type="utf8")

    lattice = acceptor @ rus_eng_fst
    cost = 0.0
    predicted_donor = ''
    if lattice.start() != pynini.NO_STATE_ID:
        lattice.project("output")
        lattice = pynini.shortestpath(lattice, unique=True)
        cost = pynini.shortestdistance(lattice, reverse=True)[lattice.start()]
        predicted_donor = pynini.shortestpath(lattice).string()

    return float(cost), predicted_donor


def check_in_english(
        donor: str, english_lexicon: Dict[str, List]
) -> int:
    """
    Check if the predicted donor is a well-formed English word

    :param donor: word
    :param english_lexicon: dictionary of Eng words; the keys are first letters of words
    :return: a dictionary of features + a list of possible donor candidates generated by rus-eng fst
    """

    found = 0
    if donor:
        first = donor[0]
        if first in english_lexicon:
            if donor in english_lexicon[first]:
                found = 1
        # if not found, check if the predicted donor is a compound consisting of true Eng words
        if not found and (" " in donor or "-" in donor):
            found_splits = 0
            if " " in donor:
                donor_splits = donor.split(" ")
            else:
                donor_splits = donor.split("-")
            for split in donor_splits:
                if not split:
                    continue
                start = split[0]
                if start in english_lexicon:
                    if split in english_lexicon[start]:
                        found_splits += 1
                if found_splits == len(donor_splits):
                    found = 1

    return found


def word_features(word: str, rus_eng_fst: pynini.Fst, eng_lexicon: Dict[str, List[str]]) -> Dict[str, float]:
    """
    Get word features:
        - cost of the Rus-Eng pair FST
        - English lexicon check
        - length of the word
    :param word: word
    :param rus_eng_fst: Rus-Eng WFST
    :param eng_lexicon: Eng lexicon dictionary, where keys are alphabet letters
                        and values are lists of words that start with that letter
    :return: a dictionary of word features
    """
    cost, donor = predict_donor(word, rus_eng_fst)
    english_check = check_in_english(donor, eng_lexicon)
    features = {
        'cost': cost,
        'eng_lex_check': float(english_check),
        'len': len(word)
    }

    return features


def main(args: argparse.Namespace) -> None:
    """
    Train and test a LogReg classifier using features from pretrained pair ngram WFSTs.
    Save test results to a csv
    """

    eng_lexicon = json.load(open(args.eng_lexicon_path))
    logging.info("loaded Eng lexicon")

    words = []
    labels = []
    with open(args.data_path) as source:
        for line in source:
            word, label = line.strip().split('\t')
            words.append(word.strip())
            labels.append(int(label.strip()))

    train_words, test_words, train_labels, test_labels = train_test_split(
        words, labels, test_size=0.1, random_state=13
    )
    logging.info("data split into train and test sets")

    fsts = [file for file in os.listdir(args.fsts_dir) if "rus_eng" in file]

    feature_dictionary = {
        'word': train_words + test_words,
        'label': train_labels + test_labels,
        'partition': ['train' for i in range(len(train_labels))] + ['test' for i in range(len(test_labels))]
    }

    keys = ['fst', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1']
    with open(args.results_path, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=keys)
        writer.writeheader()
        for fst in fsts:
            logging.info(f"training {fst}")
            rus_eng_fst = pynini.Fst.read(args.fsts_dir + fst)
            train_features = [word_features(word, rus_eng_fst, eng_lexicon) for word in train_words]
            test_features = [word_features(word, rus_eng_fst, eng_lexicon) for word in test_words]
            feature_dictionary[fst] = train_features + test_features
            logging.info("train and test features extracted")

            # train
            pipeline = make_pipeline(
                DictVectorizer(),
                StandardScaler(with_mean=False), # sparse -> no centering
                LogisticRegression(random_state=13),
            ).fit(train_features, train_labels)

            # test
            predicted = pipeline.predict(test_features)
            precision, recall, fscore, _ = precision_recall_fscore_support(test_labels, predicted)
            results = {
                "fst": fst,
                "precision_0": round(precision[0] * 100, 2),
                "precision_1": round(precision[1] * 100, 2),
                "recall_0": round(recall[0] * 100, 2),
                "recall_1": round(recall[1] * 100, 2),
                "f1_0": round(fscore[0] * 100, 2),
                "f1_1": round(fscore[1] * 100, 2),
            }
            writer.writerow(results)
            logging.info(f"{fst} logreg trained and tested: f1_1={round(fscore[1] * 100, 2)}")

    # save features
    feature_df = pd.DataFrame.from_dict(feature_dictionary)
    feature_df.to_csv(args.data_path[:-4] + '.csv')


if __name__ == "__main__":
    logging.basicConfig(level="INFO", format="%(levelname)s: %(message)s")
    parser = argparse.ArgumentParser(description="Train LogReg classifiers based on Rus-Eng WFSTs")
    parser.add_argument(
        "--data_path", required=True, help="path to input TSV file"
    )
    parser.add_argument(
        "--eng_lexicon_path", required=True, help="path to Eng lexicon json"
    )
    parser.add_argument(
        "--results_path", required=True, help="path to output csv file "
    )
    parser.add_argument(
        "--fsts_dir", required=True, help="path to directory with pretrained Rus-Eng WFSTs"
    )
    main(parser.parse_args())
